#!/usr/bin/env python3
"""
Document chunking utility for healthcare financial data.
This module handles chunking the synthetic data for optimal vector storage and retrieval.
"""

import os
import json
import logging
import pandas as pd
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.docstore.document import Document

# Configure logging
logging.basicConfig(level=logging.INFO, 
                   format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# Constants
CHUNK_SIZE = 600  # As specified in the prompt
CHUNK_OVERLAP = 90  # 15% of chunk size
DATA_DIR = os.path.join(os.path.dirname(os.path.dirname(os.path.abspath(__file__))), 'data')

# Correct file paths to match those generated by generate_synthetic_data.py
ACCOUNTS_FILE = os.path.join(DATA_DIR, 'synthetic_healthcare_accounts.csv')
TRANSACTIONS_FILE = os.path.join(DATA_DIR, 'synthetic_healthcare_transactions.csv')
PRODUCTS_FILE = os.path.join(DATA_DIR, 'synthetic_healthcare_products.json')
PLANS_FILE = os.path.join(DATA_DIR, 'synthetic_healthcare_plans.json')

def load_data():
    """Load both synthetic data and product data for chunking."""
    data_sources = []
    
    # Load synthetic healthcare account data
    if os.path.exists(ACCOUNTS_FILE):
        logger.info(f"Loading accounts data from {ACCOUNTS_FILE}")
        try:
            accounts_df = pd.read_csv(ACCOUNTS_FILE)
            accounts_text = accounts_df.to_csv(index=False)
            data_sources.append({
                "content": accounts_text,
                "metadata": {"source": "healthcare_accounts"}
            })
        except Exception as e:
            logger.error(f"Failed to load accounts data: {e}")
    else:
        logger.warning(f"Accounts file not found: {ACCOUNTS_FILE}")
    
    # Load transactions data
    if os.path.exists(TRANSACTIONS_FILE):
        logger.info(f"Loading transactions data from {TRANSACTIONS_FILE}")
        try:
            transactions_df = pd.read_csv(TRANSACTIONS_FILE)
            # Take a sample if it's too large
            if len(transactions_df) > 10000:
                transactions_df = transactions_df.sample(10000)
            transactions_text = transactions_df.to_csv(index=False)
            data_sources.append({
                "content": transactions_text,
                "metadata": {"source": "healthcare_transactions"}
            })
        except Exception as e:
            logger.error(f"Failed to load transactions data: {e}")
    else:
        logger.warning(f"Transactions file not found: {TRANSACTIONS_FILE}")
    
    # Load product data
    if os.path.exists(PRODUCTS_FILE):
        logger.info(f"Loading products data from {PRODUCTS_FILE}")
        try:
            with open(PRODUCTS_FILE, 'r') as f:
                products_data = json.load(f)
                # Process each product separately for better chunks
                for product in products_data.get("products", []):
                    product_text = json.dumps(product, indent=2)
                    data_sources.append({
                        "content": product_text,
                        "metadata": {
                            "source": "healthcare_products",
                            "product_id": product.get("id", ""),
                            "product_name": product.get("name", "")
                        }
                    })
        except Exception as e:
            logger.error(f"Failed to load products data: {e}")
    else:
        logger.warning(f"Products file not found: {PRODUCTS_FILE}")
    
    # Load plans data
    if os.path.exists(PLANS_FILE):
        logger.info(f"Loading plans data from {PLANS_FILE}")
        try:
            with open(PLANS_FILE, 'r') as f:
                plans_data = json.load(f)
                plans_text = json.dumps(plans_data, indent=2)
                data_sources.append({
                    "content": plans_text,
                    "metadata": {"source": "healthcare_plans"}
                })
        except Exception as e:
            logger.error(f"Failed to load plans data: {e}")
    else:
        logger.warning(f"Plans file not found: {PLANS_FILE}")
    
    if not data_sources:
        logger.error("No data sources could be loaded")
    else:
        logger.info(f"Successfully loaded {len(data_sources)} data sources")
    
    return data_sources

def create_chunks(data_sources):
    """
    Create optimally sized chunks from the data sources using RecursiveCharacterTextSplitter.
    
    Args:
        data_sources: List of dictionaries with content and metadata
        
    Returns:
        List of Document objects ready for vector database indexing
    """
    logger.info(f"Creating chunks with size={CHUNK_SIZE}, overlap={CHUNK_OVERLAP}")
    
    # Initialize the text splitter as specified in the prompt
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=CHUNK_SIZE,
        chunk_overlap=CHUNK_OVERLAP,
        length_function=len,
        add_start_index=True
    )
    
    all_chunks = []
    
    # Process each data source
    for source in data_sources:
        content = source["content"]
        metadata = source["metadata"]
        
        # Split the text into chunks
        texts = text_splitter.split_text(content)
        
        # Create Document objects for each chunk
        for i, text in enumerate(texts):
            # Enrich metadata with chunk information
            chunk_metadata = metadata.copy()
            chunk_metadata["chunk_id"] = i
            chunk_metadata["chunk_count"] = len(texts)
            
            # Create a Document object
            doc = Document(
                page_content=text,
                metadata=chunk_metadata
            )
            
            all_chunks.append(doc)
    
    logger.info(f"Created {len(all_chunks)} chunks from {len(data_sources)} data sources")
    return all_chunks

def process_data_for_vectorization():
    """Main function to load data and create chunks for vector database."""
    # Load data from various sources
    data_sources = load_data()
    
    if not data_sources:
        logger.error("Error: No data sources found for chunking")
        return None
    
    # Create chunks from the data
    chunks = create_chunks(data_sources)
    
    return chunks

if __name__ == "__main__":
    logger.info("--- Starting Document Chunking Process ---")
    chunks = process_data_for_vectorization()
    
    if chunks:
        logger.info(f"Successfully created {len(chunks)} chunks")
        logger.info(f"Sample chunk: {chunks[0].page_content[:100]}...")
        logger.info(f"Sample metadata: {chunks[0].metadata}")
    else:
        logger.error("Document chunking process failed")
    
    logger.info("--- Document Chunking Process Complete ---") 